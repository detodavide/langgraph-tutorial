{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwvqkvkNswZJ"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain_openai langchain_community pypdf chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2ahyBHttEXx"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "MODEL=\"gpt-4o-mini\"\n",
        "\n",
        "# Langsmith, comment away if not needed\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "# os.environ[\"LANGCHAIN_ENDPOINT\"] = userdata.get('LANGCHAIN_ENDPOINT')\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = userdata.get('LANGCHAIN_PROJECT')\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlTFkFPitGdx"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "print(f\"The current working directory is: {current_directory}\")\n",
        "\n",
        "# List contents of the current directory\n",
        "print(\"\\nContents of the current directory:\")\n",
        "for item in os.listdir(current_directory):\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "EoE1et9ZDvk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKXFya73Yb33"
      },
      "outputs": [],
      "source": [
        "file_name = \"grokkin-paper.pdf\"\n",
        "pdf_path = f\"{file_name}\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(f\"Error: The file {pdf_path} does not exist.\")\n",
        "    # You might want to raise an exception here or provide alternative paths\n",
        "    raise FileNotFoundError(f\"The file {pdf_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-AR9Q2UW5k9"
      },
      "outputs": [],
      "source": [
        "pdf_paths = [pdf_path]\n",
        "docs = []\n",
        "for path in pdf_paths:\n",
        "    loader = PyPDFLoader(path)\n",
        "    docs.extend(loader.load())\n",
        "\n",
        "# 2. Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                                                                    chunk_size=1000,\n",
        "                                                                    chunk_overlap=200,\n",
        "                                                                    )\n",
        "doc_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. Create embeddings and store in vector database\n",
        "vectorstore = Chroma.from_documents(\n",
        "                                    documents=doc_splits,\n",
        "                                    collection_name=\"pdf-rag\",\n",
        "                                    embedding=OpenAIEmbeddings(api_key=api_key),\n",
        "                                    )\n",
        "\n",
        "# 4. Create a retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuP6zgVBcs7h"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "text = \"Search paper and return information about grokkin in neural networks.\"\n",
        "retriever_tool = create_retriever_tool(\n",
        "                                        retriever,\n",
        "                                        \"retrieve_grokkin_info\",\n",
        "                                        text,\n",
        "                                        )\n",
        "\n",
        "tools = [retriever_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88D-c-aZhiX5"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMvmRp-AchFU"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "llm = ChatOpenAI(api_key=api_key,model=MODEL)\n",
        "\n",
        "\n",
        "def bot_node(state: MessagesState) -> MessagesState:\n",
        "  print(state)\n",
        "  while True:\n",
        "      try:\n",
        "          model = llm.bind_tools(tools)\n",
        "          result = model.invoke(state[\"messages\"]) # 30 seconds timeout\n",
        "\n",
        "          if not result.tool_calls and (\n",
        "              not result.content\n",
        "              or isinstance(result.content, list)\n",
        "              and not result.content[0].get(\"text\")\n",
        "          ):\n",
        "              # Add a message to request a valid response\n",
        "              messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
        "              state = {**state, \"messages\": messages}\n",
        "          else:\n",
        "              # Break the loop when valid output is obtained\n",
        "              break\n",
        "      except TimeoutError:\n",
        "          print(e)\n",
        "          # Handle timeout\n",
        "          return {\"messages\": [(\"system\", \"Operation failed, timeout error. Please try again.\")]}\n",
        "      except Exception as e:\n",
        "          # Handle other exceptions\n",
        "          print(e)\n",
        "          return {\"messages\": [(\"system\", \"An error occurred. Please try again.\")]}\n",
        "\n",
        "  # Return the final state after processing the runnable\n",
        "  return {\"messages\": result}\n",
        "\n",
        "\n",
        "graph_flow = StateGraph(MessagesState)\n",
        "graph_flow.add_node(\"chatbot\", bot_node)\n",
        "retrieve = ToolNode(tools)\n",
        "graph_flow.add_node(\"retrieve\", retrieve)\n",
        "\n",
        "graph_flow.add_conditional_edges(\n",
        "  \"chatbot\",\n",
        "  # Assess agent decision\n",
        "  tools_condition,\n",
        "  {\n",
        "      # Translate the condition outputs to nodes in our graph\n",
        "      \"tools\": \"retrieve\",\n",
        "      END: END,\n",
        "  },\n",
        ")\n",
        "\n",
        "graph_flow.add_edge(START, \"chatbot\")\n",
        "graph_flow.add_edge(\"retrieve\", \"chatbot\")\n",
        "\n",
        "\n",
        "graph = graph_flow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH0i2-tihd7C"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCxueIlPiN9s"
      },
      "outputs": [],
      "source": [
        "# Another way of building the chat\n",
        "thread_id = \"12222\"\n",
        "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nUser: \")\n",
        "    if user_input == \"q\":\n",
        "        break\n",
        "    events = graph.stream(\n",
        "        {\"messages\": [HumanMessage(content=user_input)]}, config, stream_mode=\"values\"\n",
        "    )\n",
        "    for event in events:\n",
        "        event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy-NfhYWip2S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}